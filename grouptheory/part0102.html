<h2 id="all-of-2-times-2-invertible-matrices-matrix-multiplication" class="unnumbered">( <span class="math inline">\(\{\)</span> All of 2 <span class="math inline">\(\times\)</span> 2 invertible matrices <span class="math inline">\(\}\)</span> , Matrix Multiplication)</h2>
<p>OK. As a reminder, the inverse of any matrix <span class="math inline">\(A\)</span> can be calculated as:</p>
<p><span class="math display">\[A^{-1} = \frac{1}{det(A)} adj(A) \label{eq:inverse}\]</span></p>
<p>Which <span class="math inline">\(det(A)\)</span> and <span class="math inline">\(adj(A)\)</span> are determinant and adjugate of our arbitarary <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span>. In case of our <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(A = \bigl( \begin{smallmatrix}a &amp; b\\ c &amp; d\end{smallmatrix}\bigr)\)</span>, the inverse is determined like this:</p>
<p><span class="math display">\[A^{-1} = \frac{1}{ad-bc} \begin{bmatrix}
d &amp; -b \\ -c &amp; a
\end{bmatrix}\]</span></p>
<p>You can see that if the <span class="math inline">\(ad-bc=0\)</span> or <span class="math inline">\(ad=bc\)</span>, the inverse does not exist. Actually, we know from Linear algebra that a matrix is invertible if and only if its’ determinant is not equal to zero. Now let’s check if <span class="math inline">\(2 \times 2\)</span> invertible matrices do form a group or not.</p>
<p>We know that <span class="math inline">\(det(AB)=det(A)det(B)\)</span>. By knowing this, we can see that the determinant of two non-zero matrices multiplication is also non-zero, and hence its’ matrix is a member of our set. This shows that our set is <strong>closed</strong> under matrix multiplication.</p>
<p>We know that <span class="math inline">\(I = \bigl( \begin{smallmatrix}1 &amp; 0\\ 0 &amp; 1\end{smallmatrix}\bigr)\)</span> is the <strong>neutral member</strong> of matrix multiplication. Since <span class="math inline">\(det(I)=1 \neq 0\)</span>, then <span class="math inline">\(I\)</span> is in fact a member of our set. So the first axiom is checked.</p>
<p>Do we have an <strong>inverse</strong> for all of our members? Well, Obviously yes, since we’ve selected all of the <em>invertible</em> matrices. You can see now why this certain selection was necessary to form a group out of our square matrices.</p>
<p>And finally, by knowing that the matrix multiplication is an <strong>associative operation</strong>, we prove that this is in fact a group.<br />
<br />
Before going to get more serious, let’s examine a very fun and interesting example.</p>
<h2 id="all-of-the-colors-color-combination" class="unnumbered"> ( <span class="math inline">\(\{\)</span> All of the Colors <span class="math inline">\(\}\)</span> , Color Combination)</h2>
<p>Groups are not all about numbers. They talk about any type of set or operation, and they can form in many different forms. Now let’s see if this certain set and operation is a group or not.</p>
<p>We know that all of the colors can be created out of three Red, Green and Blue. We can denote each color by their RGB (Red, Green, Blue) colors like <span class="math inline">\((r,g,b)\)</span>. We can formally define the set of all the colors as <span class="math inline">\(\{ (r,g,b) | r,g,b \in \mathbb{R} \&amp; 0&lt;r,g,b&lt;1 \}\)</span>. So <span class="math inline">\((1,0,0)\)</span> is the color red, <span class="math inline">\((1,1,1)\)</span> is the color white and <span class="math inline">\((0,0,0)\)</span> is black. When we are combining two colors, we are taking a mean out of their values of rgb, so our color combination operation is like <span class="math inline">\((r_1,g_1,b_1)(r_2,g_2,b_2)=((r_1+ r_2) / 2,(g_1+ g_2 )/ 2,(b_1+ b_2) / 2)\)</span>.</p>
<p>Based on these definitions, is this a group?</p>
<p>Well, it seems like a group at first, and it is <strong>closed</strong>, but actually it does not fulfill any of the axioms to be a group. At a first glance for an <strong>identity member</strong>, the color black (0,0,0) seems like a good idea. But it is not actually an identity member. Only the the combination of a color with itself returns the same color. But remember, for a group, a <strong>Single global</strong> identity member is required, so the first axiom remains unfulfilled.</p>
<p>You can see that <strong>inverse member</strong> is not also present in this set. What about <strong>associativity</strong>? Let’s check it for an example of White(1,1,1), gray(0.5,0.5,0.5), and black(0,0,0).</p>
<p><span class="math display">\[\bigg( (1,1,1)  (0.5,0.5,0.5) \bigg)  (0,0,0) \stackrel{?}{=} (1,1,1)  \bigg( (0.5,0.5,0.5) (0,0,0) \bigg)\]</span></p>
<p><span class="math display">\[\bigg( (0.75,0.75,0.75) \bigg)  (0,0,0) \stackrel{?}{=} (1,1,1)  \bigg( (0.25,0.25,0.25) \bigg)\]</span></p>
<p><span class="math display">\[(0.375,0.375,0.375) \neq (0.625,0.625,0.625)\]</span></p>
<p>In fact you can check that since <span class="math inline">\(\overline{(\overline{(a,b)},c)} \neq \overline{(a, \overline{(b,c)})}\)</span> shows no associativity, no operation with the form of taking a <em>mean</em> can be a legitimate group operation.</p>
<h2 id="gl_nmathbbr-all-of-n-times-n-invertible-matrices-mm" class="unnumbered"><span class="math inline">\(GL_n(\mathbb{R})=\)</span>( <span class="math inline">\(\{\)</span> All of n <span class="math inline">\(\times\)</span> n invertible matrices <span class="math inline">\(\}\)</span> , MM <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>)</h2>
<p>We have already proven the special case of <span class="math inline">\(2 \times 2\)</span>, <span class="math inline">\(GL_2(\mathbb{R})\)</span>, is in fact a group. The notation of <span class="math inline">\(\mathbb{R}\)</span> indicates that we are dealing with real matrices. GL stands for <strong>Global Linear</strong>.</p>
<p>These invertible <span class="math inline">\(n \times n\)</span> matrices represent linear transformations in a n-dimensional space. Again, by repeating the same argues for <span class="math inline">\(2 \times 2\)</span> cases, we can show that since a multiplication of two <span class="math inline">\(n \times n\)</span> matrices produce another <span class="math inline">\(n \times n\)</span> matrix, and since the determinant still remains as a non-zero value, the set is <strong>closed</strong>. The <strong>identity member</strong> is <span class="math inline">\(I_n\)</span> and the <strong>inverse</strong> is also exists for each member, according to the inverse equation. Since matrix multiplication is <strong>associative</strong>, <span class="math inline">\(GL_n(\mathbb{R})\)</span> is a group, in fact an important group, for each dimension value of <span class="math inline">\(n\)</span>.</p>
<p>If <span class="math inline">\(det(A)=1\)</span>, we call <span class="math inline">\(A\)</span> <em>special</em>. We can form another group from the set of all the <span class="math inline">\(n \times n\)</span> invertible matrices, which their determinant is 1. It is a special case of the above group. You can see that since <span class="math inline">\(det(AB)=det(A)det(B)\)</span>, if <span class="math inline">\(det(A)=det(B)=1\)</span>, then their product is also a member of this new group, and this assures the <strong>closure</strong>. The other axioms can be checked, the same as before. We call this group <em>Special Linear Group</em>, and denote it as <span class="math inline">\(SL_n(\mathbb{R})\)</span>.</p>
<p>These special matrices/transformations are important for us because they preserve the volume (length) in <span class="math inline">\(\mathbb{R}^{n}\)</span> space upon application. For example, <span class="math inline">\(SL_2(\mathbb{R})\)</span>, is actually group of rotations in 2D space in the form of <span class="math inline">\(\bigl( \begin{smallmatrix} \cos \theta &amp; \sin \theta\\ - \sin \theta &amp; \cos \theta\end{smallmatrix}\bigr)\)</span>. You know from linear algebra, that this rotation preserve the length in this 2D space.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Matrix Multiplication<a href="#fnref1" class="footnote-back" role="doc-backlink">↩</a></p></li>
</ol>
</section>
<br><br><br><br><br><br>
<h2 id="o_n-all-n-dimensional-orthogonal-matrices-mm" class="unnumbered"><span class="math inline">\(O_n=\)</span>( <span class="math inline">\(\{\)</span> All n-dimensional Orthogonal Matrices<span class="math inline">\(\}\)</span> , MM)</h2>
<p>Let’s have a reminder for the orthogonal matrices:</p>
<p>Imagine you have a n-dimensional vector <span class="math inline">\(\vec{V}\)</span>. This vector can be written in matrix form, as a column matrix, in some arbitrary basis. As we know, The absolute value squared of the vector <span class="math inline">\(\vec{V}\)</span> is denoted as <span class="math inline">\(|\vec{V}|^{2}\)</span> and can be written as <span class="math inline">\(|\vec{V}|^{2}=\vec{V}^{T} \vec{V}\)</span>, which <span class="math inline">\(\vec{V}^{T}\)</span> is the transposed <span class="math inline">\(\vec{V}\)</span>. A transpose of a matrix, is the same matrix, with its’ rows and columns switched. For instance <span class="math inline">\(\bigl( \begin{smallmatrix} a &amp; b \\ c &amp; d\end{smallmatrix}\bigr)^{T}=\bigl( \begin{smallmatrix} a &amp; c \\ b &amp; d\end{smallmatrix}\bigr)\)</span>. Now imagine we act a <span class="math inline">\(n \times n\)</span> matrix transformation <span class="math inline">\(A\)</span>, on our vector <span class="math inline">\(\vec{V}\)</span>, and we want to check what will happen to the size or the absolute value of my vector.</p>
<p><span class="math display">\[|A\vec{V}|^{2}= (A\vec{V})^{T} (A\vec{V})  =  \vec{V}^{T} A^{T} A \vec{V}\]</span></p>
<p>Which we have used the property <span class="math inline">\((A B \cdots Z)^{T} = Z^{T} \cdots B^{T} A^{T}\)</span>. Notice that Based on the above equation, if <span class="math inline">\(A^{T}=A^{-1}\)</span>, then the size of my vector is preserved under the application of <span class="math inline">\(A\)</span>. If this is true for any <span class="math inline">\(n \times n\)</span> matrix, we call that matrix an <em>Orthogonal Matrix</em>.<br />
<strong>Orthogonal matrix:</strong> <span class="math display">\[A^{T}=A^{-1}\]</span></p>
<p>So as we’ve seen, these matrices preserve the length of the vectors under transformation. It is obvious that these kinds of matrices are invertible, with their inverse <span class="math inline">\(A^{T}\)</span>. It can be proved that <span class="math inline">\(det(A^{T})=det(A)\)</span>, and based on this we have:</p>
<p><span class="math display">\[A^{T} A = I \implies\]</span></p>
<p><span class="math display">\[det(A^{T} A)=det(A^{T})det(A) = 1\]</span></p>
<p><span class="math display">\[\bigg( det(A) \bigg)^2 = 1 \implies det(A)= \pm 1\]</span></p>
<p>So the determinant of an orthogonal matrix is either <span class="math inline">\(1\)</span> or <span class="math inline">\(-1\)</span>. Notice that the opposite is not necessarily true.</p>
<p>Now let’s check if the set of all the orthogonal matrices under MM is a group. If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are orthogonal, then because <span class="math inline">\(A B (AB)^{T}= A B B^{T} A^{T}= A B B^{-1} A^{-1}=1\)</span>, then the set is <strong>closed</strong>. Since <span class="math inline">\(I_n=I_n^{T}=I_n^{-1}\)</span>, this is the <strong>neutral member</strong> of our group. Because the determinant of any orthogonal matrix is either <span class="math inline">\(+1\)</span> or <span class="math inline">\(-1\)</span> (a non-zero value), then they are <strong>invertible</strong>. And we have already seen than MM is an <strong>associative</strong> operation for a set of <span class="math inline">\(n \times n\)</span> matrices. This is in fact a group.</p>
<p>Just like the case for general linear group, we can make a separate group for the <em>special</em> cases of orthogonal matrices with their their determinant equal to 1. We call this group <em>Special Orthogonal Group</em>, in n dimensions, and denote it as <span class="math inline">\(SO_n\)</span>. Notice that it is impossible to make any different special case for the case <span class="math inline">\(det(A)=-1\)</span>, since our identity member’s determinant is 1.</p>
<h2 id="u_n-all-n-dimensional-unitary-matrices-mm" class="unnumbered"><span class="math inline">\(U_n=\)</span>( <span class="math inline">\(\{\)</span> All n-dimensional Unitary Matrices<span class="math inline">\(\}\)</span> , MM)</h2>
<p>This is analogous to the above case of orthogonal matrices, just in this case, unitary matrices are defined for complex matrices of <span class="math inline">\(\mathbb{C}\)</span>. We know that the squared absolute value of a complex number is <span class="math inline">\(|z|^{2}=z^{*} z\)</span>, which <span class="math inline">\(z^{*}\)</span> is the complex conjugate of <span class="math inline">\(z\)</span>. because of this difference, the squared absolute value of a complex vector, is defined as <span class="math inline">\(|\vec{V}|^2 = (\vec{V}^{*})^{T} \vec{V}\)</span>. We denote <span class="math inline">\((\vec{V}^{*})^{T}\)</span> as <span class="math inline">\(\vec{V}^{\dagger}\)</span> <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>1</sup></a>. We can follow the same argument as orthogonal matrices, and get the definition for a unitary matrix:<br />
<strong>Unitary matrix:</strong> <span class="math display">\[U^{\dagger}=U^{-1}\]</span></p>
<p>Orthogonal matrices are actually the real analogue of these unitary matrices. These matrices preserve norm of our complex vectors. Similarly, we can prove that <span class="math inline">\(|det(U)|= 1\)</span>, but this time since U is complex (hence <span class="math inline">\(det(U)\)</span> is also complex), the <span class="math inline">\(det(U)= e ^{i \phi}\)</span>, with <span class="math inline">\(0&lt;\phi&lt;2 \pi\)</span> as a phase in our complex plane.</p>
<p>Now is the set of our <span class="math inline">\(n \times n\)</span> unitary matrices a group under MM? Using a similar reasoning to the orthogonal case, we can assure its’ <strong>closure</strong>. The <strong>identity member</strong> is the same <span class="math inline">\(I_n\)</span>. Since <span class="math inline">\(det(U)= e ^{i \phi} \neq 0\)</span>, the <strong>inverse member</strong> exists for each of the unitary matrices. The MM is still an <strong>associative</strong> operation for complex matrices of <span class="math inline">\(\mathbb{C}\)</span>. Based on all of these, we can see that our <span class="math inline">\(U_n\)</span> is also a group. Again for the <em>special</em> case of <span class="math inline">\(det(U)=1\)</span>, we are having another group called <span class="math inline">\(SU_n\)</span>.<br />
<br />
So, we’ve talked about many forms of <span class="math inline">\(n \times n\)</span> matrices as a linear transformation, that with MM, forms a group. These groups are very important, and we are going to come back to them many times, so try to memorize their notations. The table below summarize these groups that we’ve talked about, and can assist you to remember everything we’ve just mentioned.</p>

<img src="Img\Table1-1.png" alt="Table 1-1" height="241" width="435" class="center"> 

<p>OK, let’s continue our examples, but this time, let’s change the subject from matrices.</p>
<br><br><br>

<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn2" role="doc-endnote"><p>Different people have different notations for this, but physicist usually use this notation, a deformed T called dagger <span class="math inline">\(\dagger\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩</a></p></li>
</ol>
</section>


